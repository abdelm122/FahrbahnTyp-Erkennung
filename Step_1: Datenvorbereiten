# 01_datenvorbereiten.py
# ------------------------------------------------------------
# Step 1: Daten vorbereiten
# - Liest ATOMS3-CSV-Dateien aus Klassen-Ordnern (z.B. trabi, 3adi, mobalat)
# - Standardisiert Spaltennamen
# - Sortiert nach Timestamp, entfernt ungültige Werte
# - Schneidet feste Zeitfenster (Windows) aus den Signalen
# - Erstellt Train/Val/Test Split OHNE Leakage (Split nach Datei = "Group Split")
# - Speichert train_raw.npz / val_raw.npz / test_raw.npz + metadata.json
#
# Erwartete Ordnerstruktur (Beispiel):
# C:\Users\Oulad\Downloads\LOP\
#   trabi\   *.csv
#   3adi\    *.csv
#   mobalat\ *.csv
#
# CSV Spalten (wie du beschrieben hast):
# Timestamp(ms) Date Time AX(g) AY(g) AZ(g) GX(deg/s) GY(deg/s) GZ(deg/s)
# ------------------------------------------------------------

import argparse
import json
import os
from dataclasses import dataclass
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd

try:
    from sklearn.model_selection import GroupShuffleSplit
except ImportError as e:
    raise SystemExit(
        "scikit-learn fehlt. Installiere es mit:\n"
        "pip install scikit-learn"
    ) from e


# ----------------------------- Konfiguration -----------------------------

SENSOR_COLS_STD = ["ax_g", "ay_g", "az_g", "gx_dps", "gy_dps", "gz_dps"]

COLUMN_ALIASES = {
    # Timestamp
    "timestamp(ms)": "timestamp_ms",
    "timestamp": "timestamp_ms",
    "timestamp_ms": "timestamp_ms",

    # Accelerometer
    "ax(g)": "ax_g",
    "ay(g)": "ay_g",
    "az(g)": "az_g",
    "ax": "ax_g",
    "ay": "ay_g",
    "az": "az_g",

    # Gyro
    "gx(deg/s)": "gx_dps",
    "gy(deg/s)": "gy_dps",
    "gz(deg/s)": "gz_dps",
    "gx": "gx_dps",
    "gy": "gy_dps",
    "gz": "gz_dps",
}

# Einige CSVs kommen gern mit merkwürdigen Leerzeichen oder BOM
def _normalize_colname(c: str) -> str:
    c = c.strip().replace("\ufeff", "")
    c = c.replace(" ", "")
    return c.lower()


@dataclass
class WindowConfig:
    window_seconds: float = 2.0       # Fensterlänge in Sekunden
    hop_seconds: float = 1.0          # Schrittweite in Sekunden
    min_windows_per_file: int = 1     # Dateien ohne genügend Windows werden übersprungen
    max_windows_per_file: int = 10_000  # Schutz gegen Ausreißer


# ----------------------------- I/O Hilfen -----------------------------

def list_class_folders(data_dir: str) -> List[str]:
    """Findet Unterordner (Klassen) direkt unter data_dir."""
    if not os.path.isdir(data_dir):
        raise FileNotFoundError(f"data_dir existiert nicht: {data_dir}")

    classes = []
    for name in sorted(os.listdir(data_dir)):
        p = os.path.join(data_dir, name)
        if os.path.isdir(p):
            classes.append(name)
    if not classes:
        raise RuntimeError(f"Keine Klassen-Ordner gefunden in: {data_dir}")
    return classes


def discover_csv_files(data_dir: str) -> List[Tuple[str, str]]:
    """
    Liefert Liste (class_name, filepath) für alle CSV-Dateien unter data_dir/<class_name>/.
    """
    pairs = []
    classes = list_class_folders(data_dir)
    for cls in classes:
        cls_dir = os.path.join(data_dir, cls)
        for root, _, files in os.walk(cls_dir):
            for fn in files:
                if fn.lower().endswith(".csv"):
                    pairs.append((cls, os.path.join(root, fn)))
    if not pairs:
        raise RuntimeError(f"Keine CSV-Dateien gefunden unter: {data_dir}")
    return pairs


def read_csv_robust(path: str) -> pd.DataFrame:
    """
    Robust gegen:
    - Trennzeichen ; oder ,
    - Dezimal-Komma
    - gemischte Header-Formate
    """
    # Versuch 1: Standard (Komma)
    try:
        df = pd.read_csv(path, engine="python")
        if df.shape[1] <= 1:
            # vermutlich Semikolon-getrennt
            df = pd.read_csv(path, sep=";", engine="python")
    except Exception:
        # Fallback: Semikolon
        df = pd.read_csv(path, sep=";", engine="python")

    # Wenn Dezimal-Komma vorkommt, konvertieren wir später numerisch mit errors='coerce'
    return df


def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    Normalisiert Spaltennamen auf:
    timestamp_ms, ax_g, ay_g, az_g, gx_dps, gy_dps, gz_dps
    """
    # Mappe Spaltennamen
    new_cols = {}
    for c in df.columns:
        norm = _normalize_colname(str(c))
        if norm in COLUMN_ALIASES:
            new_cols[c] = COLUMN_ALIASES[norm]

    df = df.rename(columns=new_cols)

    required = ["timestamp_ms"] + SENSOR_COLS_STD
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"Fehlende Spalten: {missing}. Vorhanden: {list(df.columns)}")

    # Nur relevante Spalten behalten
    df = df[required].copy()

    # Numerische Konvertierung (inkl. Dezimal-Komma)
    for c in required:
        if c == "timestamp_ms":
            df[c] = pd.to_numeric(df[c], errors="coerce")
        else:
            # Dezimal-Komma -> Punkt, dann numeric
            df[c] = df[c].astype(str).str.replace(",", ".", regex=False)
            df[c] = pd.to_numeric(df[c], errors="coerce")

    # Entferne Zeilen mit NaNs
    df = df.dropna(axis=0).reset_index(drop=True)

    # Sortiere nach Zeit
    df = df.sort_values("timestamp_ms").reset_index(drop=True)

    return df


def estimate_sampling_rate_hz(timestamp_ms: np.ndarray) -> float:
    """
    Schätzt Sampling-Rate aus Timestamp-Differenzen (Median).
    """
    if len(timestamp_ms) < 5:
        return np.nan
    dt_ms = np.diff(timestamp_ms)
    dt_ms = dt_ms[(dt_ms > 0) & (dt_ms < 10_000)]  # Filter gegen Ausreißer
    if len(dt_ms) < 3:
        return np.nan
    median_dt = np.median(dt_ms)
    return float(1000.0 / median_dt) if median_dt > 0 else np.nan


# ----------------------------- Windowing -----------------------------

def make_windows(
    data: np.ndarray,
    fs_hz: float,
    cfg: WindowConfig
) -> np.ndarray:
    """
    Schneidet Windows aus data (shape: [N, 6]) mit Länge window_seconds.
    Rückgabe: windows (shape: [num_windows, window_len, 6])
    """
    if not np.isfinite(fs_hz) or fs_hz <= 0:
        raise ValueError("Sampling-Rate konnte nicht geschätzt werden (fs_hz ungültig).")

    window_len = int(round(cfg.window_seconds * fs_hz))
    hop_len = int(round(cfg.hop_seconds * fs_hz))

    if window_len < 8:
        raise ValueError(f"window_len zu klein ({window_len}). Erhöhe window_seconds oder prüfe fs_hz.")
    if hop_len < 1:
        hop_len = 1

    n = data.shape[0]
    if n < window_len:
        return np.empty((0, window_len, data.shape[1]), dtype=np.float32)

    starts = np.arange(0, n - window_len + 1, hop_len, dtype=int)
    if len(starts) > cfg.max_windows_per_file:
        starts = starts[: cfg.max_windows_per_file]

    windows = np.stack([data[s:s + window_len] for s in starts], axis=0).astype(np.float32)
    return windows


# ----------------------------- Splitting -----------------------------

def group_split(
    X: np.ndarray,
    y: np.ndarray,
    groups: np.ndarray,
    test_size: float = 0.15,
    val_size: float = 0.15,
    seed: int = 42
) -> Dict[str, np.ndarray]:
    """
    Split ohne Leakage:
    - Erst Test abtrennen
    - Dann aus Rest Train/Val
    """
    gss1 = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)
    trainval_idx, test_idx = next(gss1.split(X, y, groups=groups))

    X_trainval, y_trainval, g_trainval = X[trainval_idx], y[trainval_idx], groups[trainval_idx]
    X_test, y_test = X[test_idx], y[test_idx]

    # val Anteil relativ auf trainval
    val_rel = val_size / (1.0 - test_size)
    gss2 = GroupShuffleSplit(n_splits=1, test_size=val_rel, random_state=seed + 1)
    train_idx, val_idx = next(gss2.split(X_trainval, y_trainval, groups=g_trainval))

    return {
        "X_train": X_trainval[train_idx],
        "y_train": y_trainval[train_idx],
        "X_val": X_trainval[val_idx],
        "y_val": y_trainval[val_idx],
        "X_test": X_test,
        "y_test": y_test,
    }


# ----------------------------- Main Pipeline -----------------------------

def build_dataset(data_dir: str, out_dir: str, cfg: WindowConfig, seed: int = 42) -> None:
    os.makedirs(out_dir, exist_ok=True)

    pairs = discover_csv_files(data_dir)
    class_names = sorted(list(set([c for c, _ in pairs])))
    class_to_id = {c: i for i, c in enumerate(class_names)}

    all_windows = []
    all_labels = []
    all_groups = []  # group = file id (gegen leakage)
    file_summaries = []

    file_id = 0
    skipped_files = 0

    for cls, path in pairs:
        try:
            df = read_csv_robust(path)
            df = standardize_columns(df)

            ts = df["timestamp_ms"].to_numpy(dtype=float)
            fs_hz = estimate_sampling_rate_hz(ts)

            sig = df[SENSOR_COLS_STD].to_numpy(dtype=np.float32)

            windows = make_windows(sig, fs_hz, cfg)
            if windows.shape[0] < cfg.min_windows_per_file:
                skipped_files += 1
                continue

            label = class_to_id[cls]
            all_windows.append(windows)
            all_labels.append(np.full((windows.shape[0],), label, dtype=np.int64))
            all_groups.append(np.full((windows.shape[0],), file_id, dtype=np.int64))

            file_summaries.append({
                "file_id": file_id,
                "class": cls,
                "path": path,
                "fs_hz_est": fs_hz,
                "num_samples": int(sig.shape[0]),
                "num_windows": int(windows.shape[0]),
                "window_seconds": cfg.window_seconds,
                "hop_seconds": cfg.hop_seconds,
            })

            file_id += 1

        except Exception as e:
            # Robust: eine kaputte Datei soll nicht alles stoppen
            skipped_files += 1
            file_summaries.append({
                "file_id": None,
                "class": cls,
                "path": path,
                "error": str(e),
            })

    if not all_windows:
        raise RuntimeError("Keine Windows erzeugt. Prüfe deine CSVs und die Window-Parameter.")

    X = np.concatenate(all_windows, axis=0)
    y = np.concatenate(all_labels, axis=0)
    groups = np.concatenate(all_groups, axis=0)

    splits = group_split(X, y, groups, test_size=0.15, val_size=0.15, seed=seed)

    # Speichern: RAW (ohne Offset Removal und ohne Normalisierung)
    np.savez_compressed(os.path.join(out_dir, "train_raw.npz"), X=splits["X_train"], y=splits["y_train"])
    np.savez_compressed(os.path.join(out_dir, "val_raw.npz"), X=splits["X_val"], y=splits["y_val"])
    np.savez_compressed(os.path.join(out_dir, "test_raw.npz"), X=splits["X_test"], y=splits["y_test"])

    meta = {
        "data_dir": data_dir,
        "out_dir": out_dir,
        "classes": class_names,
        "class_to_id": class_to_id,
        "sensor_columns": SENSOR_COLS_STD,
        "window_seconds": cfg.window_seconds,
        "hop_seconds": cfg.hop_seconds,
        "split": {"train": float(len(splits["y_train"])), "val": float(len(splits["y_val"])), "test": float(len(splits["y_test"]))},
        "num_files_used": int(file_id),
        "num_files_skipped": int(skipped_files),
        "total_windows": int(X.shape[0]),
        "window_shape": [int(X.shape[1]), int(X.shape[2])],
        "file_summaries": file_summaries[:200],  # nicht unendlich groß machen
        "note": "RAW dataset. Step 2 macht Offset Removal + Normalisierung.",
    }

    with open(os.path.join(out_dir, "metadata.json"), "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    # Kurzer Report (Deutsch)
    print("Step 1 abgeschlossen.")
    print(f"Gefundene Klassen: {class_names}")
    print(f"Benutzte Dateien: {file_id} | Übersprungene Dateien: {skipped_files}")
    print(f"Gesamt-Windows: {X.shape[0]} | Window-Shape: {X.shape[1]} x {X.shape[2]}")
    print(f"Train: {splits['X_train'].shape[0]} | Val: {splits['X_val'].shape[0]} | Test: {splits['X_test'].shape[0]}")
    print(f"Gespeichert in: {out_dir}")
    print("Outputs: train_raw.npz, val_raw.npz, test_raw.npz, metadata.json")


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Step 1: Daten vorbereiten (ATOMS3 IMU CSVs)")
    p.add_argument(
        "--data_dir",
        type=str,
        default=r"*******",   #Hier muss du den Ordner einfügen (****)
        help="Ordner mit Klassen-Unterordnern (z.B. trabi, 3adi, mobalat)."
    )
    p.add_argument(
        "--out_dir",
        type=str,
        default=r"./artifacts_step1",
        help="Output-Ordner für vorbereitete Daten."
    )
    p.add_argument("--window_seconds", type=float, default=2.0, help="Fensterlänge in Sekunden.")
    p.add_argument("--hop_seconds", type=float, default=1.0, help="Schrittweite in Sekunden.")
    p.add_argument("--seed", type=int, default=42, help="Random Seed.")
    return p.parse_args()


if __name__ == "__main__":
    args = parse_args()
    cfg = WindowConfig(window_seconds=args.window_seconds, hop_seconds=args.hop_seconds)
    build_dataset(args.data_dir, args.out_dir, cfg, seed=args.seed)
