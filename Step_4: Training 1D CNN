import os
import re
import numpy as np
import pandas as pd
import tensorflow as tf

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# =========================
# PATHS
# =========================
INPUT_DIR = r"C:\Users\Oulad\Downloads\Devi\Devi_step3"
OUT_DIR = r"C:\Users\Oulad\Downloads\Devi\Devi_step4_cnn_leakcheck"
if not os.path.exists(OUT_DIR):
    os.makedirs(OUT_DIR)

MODEL_PATH = os.path.join(OUT_DIR, "tiny_cnn_1d.keras")

# =========================
# SETTINGS
# =========================
SENSOR_COLS = ["AX(g)", "AY(g)", "AZ(g)", "GX(deg/s)", "GY(deg/s)", "GZ(deg/s)"]

WINDOW_SIZE = 256
STRIDE = 256  # no overlap, realistischer

RANDOM_STATE = 42
EPOCHS = 25
BATCH_SIZE = 64
LEARNING_RATE = 1e-3

# =========================
# ROBUST BASE ID
# =========================
def get_base_id(filename):
    """
    Removes augmentation / orig markers robustly.
    Examples:
      abc__aug1.csv -> abc
      abc_aug2.csv  -> abc
      abc-aug3.csv  -> abc
      abc__orig.csv -> abc
      abc_orig.csv  -> abc
    """
    name = filename.lower().replace(".csv", "")

    # remove common suffix patterns
    name = re.sub(r"(__|_|-)?aug\d+$", "", name)        # __aug1 / _aug2 / -aug3
    name = re.sub(r"(__|_|-)?orig$", "", name)          # __orig / _orig / -orig
    name = re.sub(r"(__|_|-)?original$", "", name)      # original variants
    name = re.sub(r"(__|_|-)?shift\d+$", "", name)      # shift variants
    name = re.sub(r"(__|_|-)?noise\d+$", "", name)      # noise variants
    name = re.sub(r"(__|_|-)?scale\d+$", "", name)      # scale variants

    # if your pipeline uses "__" for separating parts, keep only first token
    if "__" in name:
        name = name.split("__")[0]

    return name

def is_aug_file(filename):
    fn = filename.lower()
    return ("aug" in fn) and (("aug" in fn) or ("__aug" in fn) or ("_aug" in fn) or ("-aug" in fn))

# =========================
# LOAD WINDOWS
# =========================
def load_windows_from_file(csv_path):
    try:
        df = pd.read_csv(csv_path)
    except:
        return None

    for c in SENSOR_COLS:
        if c not in df.columns:
            return None

    sig = df[SENSOR_COLS].astype(np.float32).to_numpy()
    n = len(sig)
    if n < WINDOW_SIZE:
        return None

    windows = []
    start = 0
    while start + WINDOW_SIZE <= n:
        windows.append(sig[start:start + WINDOW_SIZE, :])
        start += STRIDE

    if len(windows) == 0:
        return None

    return np.stack(windows, axis=0)  # (Nw, W, 6)

def collect_files(input_dir):
    items = []
    labels = [d for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d))]
    labels = sorted(labels)

    for lab in labels:
        lab_dir = os.path.join(input_dir, lab)
        for fn in os.listdir(lab_dir):
            if not fn.endswith(".csv"):
                continue
            fp = os.path.join(lab_dir, fn)
            base_id = get_base_id(fn)
            aug = is_aug_file(fn)
            items.append((lab, fn, fp, base_id, aug))
    return items, labels

def build_Xy(items, allowed_base_ids, use_aug, use_orig, label_to_id):
    X_list = []
    y_list = []
    base_list = []

    for lab, fn, fp, base_id, aug in items:
        if base_id not in allowed_base_ids:
            continue
        if aug and not use_aug:
            continue
        if (not aug) and not use_orig:
            continue

        Xw = load_windows_from_file(fp)
        if Xw is None:
            continue

        X_list.append(Xw)
        y_list += [label_to_id[lab]] * len(Xw)
        base_list += [base_id] * len(Xw)

    if len(X_list) == 0:
        return None, None, None

    X = np.concatenate(X_list, axis=0).astype(np.float32)
    y = np.array(y_list, dtype=np.int32)
    bases = np.array(base_list)
    return X, y, bases

# =========================
# DUPLICATE WINDOW CHECK (Hash)
# =========================
def hash_windows(X):
    # hash each window bytes to detect exact duplicates across splits
    # X shape: (N, W, C) float32
    # Convert to bytes. This detects exact same floats.
    return {hash(x.tobytes()) for x in X}

# =========================
# MODEL
# =========================
def make_tiny_cnn(input_shape, n_classes):
    inp = tf.keras.Input(shape=input_shape)

    x = tf.keras.layers.Conv1D(16, 7, padding="same", use_bias=False)(inp)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)
    x = tf.keras.layers.MaxPool1D(2)(x)

    x = tf.keras.layers.Conv1D(32, 5, padding="same", use_bias=False)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)
    x = tf.keras.layers.MaxPool1D(2)(x)

    x = tf.keras.layers.Conv1D(48, 3, padding="same", use_bias=False)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)

    x = tf.keras.layers.GlobalAveragePooling1D()(x)
    x = tf.keras.layers.Dropout(0.35)(x)
    x = tf.keras.layers.Dense(32, activation="relu")(x)
    x = tf.keras.layers.Dropout(0.25)(x)

    out = tf.keras.layers.Dense(n_classes, activation="softmax")(x)
    return tf.keras.Model(inp, out)

# =========================
# MAIN
# =========================
np.random.seed(RANDOM_STATE)
tf.random.set_seed(RANDOM_STATE)

items, label_list = collect_files(INPUT_DIR)
print("Labels:", label_list)

label_to_id = {lab: i for i, lab in enumerate(label_list)}
n_classes = len(label_list)

# base list from ORIG only
base_to_label = {}
for lab, fn, fp, base_id, aug in items:
    if aug:
        continue
    base_to_label[base_id] = lab

base_ids = sorted(list(base_to_label.keys()))
if len(base_ids) == 0:
    raise SystemExit("❌ No base_ids from original files. Check naming and is_aug_file().")

base_y = np.array([label_to_id[base_to_label[b]] for b in base_ids], dtype=np.int32)

train_bases, test_bases = train_test_split(
    np.array(base_ids),
    test_size=0.2,
    random_state=RANDOM_STATE,
    stratify=base_y
)

train_ids = set(train_bases.tolist())
test_ids = set(test_bases.tolist())

print("Train base_ids:", len(train_ids))
print("Test  base_ids:", len(test_ids))
print("Leakage check (base_id overlap):", len(train_ids.intersection(test_ids)))
assert len(train_ids.intersection(test_ids)) == 0

# Train uses orig+aug, Test uses orig only
X_train, y_train, train_base_windows = build_Xy(items, train_ids, use_aug=True, use_orig=True, label_to_id=label_to_id)
X_test, y_test, test_base_windows = build_Xy(items, test_ids, use_aug=False, use_orig=True, label_to_id=label_to_id)

if X_train is None or X_test is None:
    raise SystemExit("❌ Could not build windows. Check CSV columns and WINDOW_SIZE.")

print("Train windows:", len(X_train), "Test windows:", len(X_test))
print("Input shape:", X_train.shape)

# Duplicate exact-window leakage check
h_tr = hash_windows(X_train)
h_te = hash_windows(X_test)
dup = len(h_tr.intersection(h_te))
print("Exact duplicate windows Train∩Test:", dup)
if dup > 0:
    print("❌ Leakage confirmed: identical windows appear in both Train and Test.")

# class weights
counts = np.bincount(y_train, minlength=n_classes).astype(np.float32)
total = float(np.sum(counts))
class_weight = {i: (total / (n_classes * counts[i])) if counts[i] > 0 else 1.0 for i in range(n_classes)}
print("Class weights:", class_weight)

# tf.data
train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))
train_ds = train_ds.shuffle(8000, seed=RANDOM_STATE, reshuffle_each_iteration=True).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))
test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# train
model = make_tiny_cnn((WINDOW_SIZE, len(SENSOR_COLS)), n_classes)
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor="val_accuracy", patience=5, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(monitor="val_accuracy", factor=0.5, patience=2, min_lr=1e-5),
]

model.fit(train_ds, validation_data=test_ds, epochs=EPOCHS, class_weight=class_weight, callbacks=callbacks, verbose=1)

# eval
y_prob = model.predict(test_ds, verbose=0)
y_pred = np.argmax(y_prob, axis=1)

acc = accuracy_score(y_test, y_pred)
print("\n✅ TEST Window-Accuracy:", acc)
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=label_list))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

model.save(MODEL_PATH)
print("\n✅ Saved model:", MODEL_PATH)


