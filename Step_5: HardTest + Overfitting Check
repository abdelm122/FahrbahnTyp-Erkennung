import os
import numpy as np
import pandas as pd
import joblib

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ============================================================
# STEP 5: Hard Test + Overfitting Test (Decision Tree tuned)
# - Window-Level Eval: Train vs Test
# - File-Level Eval: Majority Vote (pro base_id)
# - Leakage check
# ============================================================

# ===== Paths (anpassen) =====
STEP3_DIR = r"C:\Users\Oulad\Downloads\Devi\Devi_step3"
STEP4_DIR = r"C:\Users\Oulad\Downloads\Devi\Devi_step4_dt_tuned"

MODEL_PATH = os.path.join(STEP4_DIR, "decision_tree_tuned.joblib")
TRAIN_BASES_PATH = os.path.join(STEP4_DIR, "train_base_ids.csv")
TEST_BASES_PATH = os.path.join(STEP4_DIR, "test_base_ids.csv")

# ===== Base columns (wie in Code 4) =====
BASE_COLS = ["AX(g)", "AY(g)", "AZ(g)", "GX(deg/s)", "GY(deg/s)", "GZ(deg/s)"]

# ===== Windowing (muss gleich wie Training sein) =====
WINDOW_SIZE = 256
STRIDE = 128

# ===== Features (muss gleich wie Training sein) =====
def features_1d(x):
    x = np.asarray(x, dtype=float)
    mean = np.mean(x)
    std = np.std(x)              # ddof=0
    mn = np.min(x)
    mx = np.max(x)
    rms = np.sqrt(np.mean(x * x))
    med = np.median(x)
    q25 = np.percentile(x, 25)
    q75 = np.percentile(x, 75)
    iqr = q75 - q25
    energy = np.mean(x * x)
    return [mean, std, mn, mx, rms, med, iqr, energy]

feat_names_1d = ["mean", "std", "min", "max", "rms", "median", "iqr", "energy"]

def get_base_id(filename):
    name = filename.replace(".csv", "")
    return name.split("__")[0]

def build_channels(df):
    AX = df["AX(g)"].astype(float).to_numpy()
    AY = df["AY(g)"].astype(float).to_numpy()
    AZ = df["AZ(g)"].astype(float).to_numpy()
    GX = df["GX(deg/s)"].astype(float).to_numpy()
    GY = df["GY(deg/s)"].astype(float).to_numpy()
    GZ = df["GZ(deg/s)"].astype(float).to_numpy()

    acc_mag = np.sqrt(AX*AX + AY*AY + AZ*AZ)
    gyro_mag = np.sqrt(GX*GX + GY*GY + GZ*GZ)

    chans = {
        "AX": AX, "AY": AY, "AZ": AZ,
        "GX": GX, "GY": GY, "GZ": GZ,
        "acc_mag": acc_mag,
        "gyro_mag": gyro_mag
    }

    # jerk
    for k in list(chans.keys()):
        x = chans[k]
        chans[k + "_jerk"] = np.diff(x, prepend=x[0])

    return chans

def make_feature_names():
    # muss gleiche Reihenfolge sein wie training
    # Reihenfolge basiert auf dict insertion order in build_channels + jerk loop
    # Python 3.7+ behält insertion order
    base_names = ["AX", "AY", "AZ", "GX", "GY", "GZ", "acc_mag", "gyro_mag"]
    jerk_names = [n + "_jerk" for n in base_names]
    ch_names = base_names + jerk_names

    names = []
    for cn in ch_names:
        for fn in feat_names_1d:
            names.append(cn + "_" + fn)
    return names, ch_names

FEATURE_NAMES, CHANNEL_NAMES = make_feature_names()

def extract_windows_X(file_path):
    try:
        df = pd.read_csv(file_path)
    except:
        return None

    for c in BASE_COLS:
        if c not in df.columns:
            return None

    chans = build_channels(df)

    # sicherstellen: wir nehmen nur die channels in gleicher Reihenfolge wie training
    n = len(df)
    if n < WINDOW_SIZE:
        return None

    X = []
    start = 0
    while start + WINDOW_SIZE <= n:
        feats = []
        for cn in CHANNEL_NAMES:
            x = chans[cn][start:start + WINDOW_SIZE]
            feats += features_1d(x)
        X.append(feats)
        start += STRIDE

    if len(X) == 0:
        return None

    return np.array(X, dtype=float)

def majority_vote(labels):
    counts = {}
    for lab in labels:
        counts[lab] = counts.get(lab, 0) + 1

    best = None
    best_n = -1
    for k in counts:
        if counts[k] > best_n:
            best = k
            best_n = counts[k]
    return best

def collect_files_for_base_ids(base_ids_set):
    # returns list of (true_label, base_id, file_path)
    items = []
    for label in os.listdir(STEP3_DIR):
        label_path = os.path.join(STEP3_DIR, label)
        if not os.path.isdir(label_path):
            continue

        for file in os.listdir(label_path):
            if not file.endswith(".csv"):
                continue

            base_id = get_base_id(file)
            if base_id in base_ids_set:
                items.append((label, base_id, os.path.join(label_path, file)))
    return items

def eval_window_level(model, items):
    y_true, y_pred = [], []
    used_files, used_windows = 0, 0

    for true_label, base_id, fp in items:
        Xw = extract_windows_X(fp)
        if Xw is None:
            continue

        pred = model.predict(Xw)

        y_true += [true_label] * len(pred)
        y_pred += list(pred)

        used_files += 1
        used_windows += len(pred)

    return y_true, y_pred, used_files, used_windows

def eval_file_level(model, items):
    # merge all window preds across all files sharing same base_id
    base_true = {}
    base_preds = {}

    for true_label, base_id, fp in items:
        Xw = extract_windows_X(fp)
        if Xw is None:
            continue

        pred = model.predict(Xw)

        base_true[base_id] = true_label
        if base_id not in base_preds:
            base_preds[base_id] = []
        base_preds[base_id] += list(pred)

    y_true, y_pred = [], []
    for base_id in base_preds:
        preds = base_preds[base_id]
        if len(preds) == 0:
            continue
        y_true.append(base_true[base_id])
        y_pred.append(majority_vote(preds))

    return y_true, y_pred, len(base_preds)

# =========================
# MAIN
# =========================
print("Loading model:", MODEL_PATH)
model = joblib.load(MODEL_PATH)

train_df = pd.read_csv(TRAIN_BASES_PATH)
test_df = pd.read_csv(TEST_BASES_PATH)

train_ids = set(train_df["base_id"].astype(str).tolist())
test_ids = set(test_df["base_id"].astype(str).tolist())

print("Train base_ids:", len(train_ids))
print("Test base_ids:", len(test_ids))
print("Leakage check (muss 0 sein):", len(train_ids.intersection(test_ids)))

train_items = collect_files_for_base_ids(train_ids)
test_items = collect_files_for_base_ids(test_ids)

print("\nTrain files found:", len(train_items))
print("Test files found:", len(test_items))

# -------------------------
# Window-Level (Overfitting)
# -------------------------
print("\n================ WINDOW-LEVEL EVALUATION ================")

y_true_tr, y_pred_tr, nfiles_tr, nw_tr = eval_window_level(model, train_items)
y_true_te, y_pred_te, nfiles_te, nw_te = eval_window_level(model, test_items)

acc_tr = accuracy_score(y_true_tr, y_pred_tr) if len(y_true_tr) else 0.0
acc_te = accuracy_score(y_true_te, y_pred_te) if len(y_true_te) else 0.0

print("\nTRAIN Window-Accuracy:", acc_tr)
print("Used train files:", nfiles_tr, " windows:", nw_tr)

print("\nTEST Window-Accuracy:", acc_te)
print("Used test files:", nfiles_te, " windows:", nw_te)

print("\nOverfitting Gap (train - test):", acc_tr - acc_te)

print("\n--- TEST Classification Report (Window-Level) ---")
print(classification_report(y_true_te, y_pred_te))

print("TEST Confusion Matrix (Window-Level):")
print(confusion_matrix(y_true_te, y_pred_te))

# -------------------------
# File-Level (Hard Test)
# -------------------------
print("\n================ FILE-LEVEL (MAJORITY VOTE) ================")

y_true_tr_f, y_pred_tr_f, nb_tr = eval_file_level(model, train_items)
y_true_te_f, y_pred_te_f, nb_te = eval_file_level(model, test_items)

acc_tr_f = accuracy_score(y_true_tr_f, y_pred_tr_f) if len(y_true_tr_f) else 0.0
acc_te_f = accuracy_score(y_true_te_f, y_pred_te_f) if len(y_true_te_f) else 0.0

print("\nTRAIN File-Accuracy:", acc_tr_f, " (base files:", nb_tr, ")")
print("TEST  File-Accuracy:", acc_te_f, " (base files:", nb_te, ")")

print("\nOverfitting Gap FILE (train - test):", acc_tr_f - acc_te_f)

print("\n--- TEST Classification Report (File-Level) ---")
print(classification_report(y_true_te_f, y_pred_te_f))

print("TEST Confusion Matrix (File-Level):")
print(confusion_matrix(y_true_te_f, y_pred_te_f))

print("\n✅ Step 5 fertig (Decision Tree Hard Test + Overfitting)")
