# 06_optimize_and_hardtest.py
# ------------------------------------------------------------
#
# Ziel:
# - etwas robusteres Modell als Step 4
# - Feature Engineering (Magnitude + optional Jerk Magnitude)
# - optional Focal Loss
# - Class Weights (empfohlen)
# - Evaluation + Hardtests + Report (Deutsch)
#
# Inputs:
#   STEP_3/npz/train_aug_x3.npz
#   STEP_3/npz/val_norm.npz
#   STEP_3/npz/test_norm.npz
#   STEP_1/metadata.json (optional)
#
# Outputs:
#   STEP_6/best_model.keras
#   STEP_6/model_float.tflite
#   STEP_6/report_de.txt
#   STEP_6/confusion_matrix.csv
#   STEP_6/hardtests.csv
#   STEP_6/metrics.json
#
# Installation:
#   pip install tensorflow numpy scikit-learn matplotlib
# ------------------------------------------------------------

import os
import json
import csv
import numpy as np

# TensorFlow
try:
    import tensorflow as tf
except ImportError:
    raise SystemExit("TensorFlow fehlt. Installiere: pip install tensorflow")

# sklearn Metrics
try:
    from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support
except ImportError:
    raise SystemExit("scikit-learn fehlt. Installiere: pip install scikit-learn")


# =========================
# EINSTELLUNGEN (HIER KANNST DU ANPASSEN)
# =========================
STEP1_META = r"./STEP_1/metadata.json"
IN_DIR = r"./STEP_3/npz"
OUT_DIR = r"./STEP_6"

# Feature Mode: "basic", "mag", "mag_jerk"
FEATURE_MODE = "mag_jerk"

# Training
SEED = 7
EPOCHS = 90
BATCH_SIZE = 128
LR = 1e-3

# Loss
USE_FOCAL = True
FOCAL_GAMMA = 2.0
FOCAL_ALPHA = 0.25

# Hardtests
HARDTEST_SEED = 202
PRED_BATCH = 256


# =========================
# BASIS I/O
# =========================
def load_npz(path):
    if not os.path.isfile(path):
        raise FileNotFoundError(f"Datei nicht gefunden: {path}")
    d = np.load(path)
    if "X" not in d or "y" not in d:
        raise ValueError(f"NPZ muss Keys X und y enthalten: {path}")
    X = d["X"].astype(np.float32, copy=False)
    y = d["y"].astype(np.int64, copy=False)
    if X.ndim != 3:
        raise ValueError(f"X muss [N,T,C] sein. Bekommen: {X.shape}")
    if y.ndim != 1 or y.shape[0] != X.shape[0]:
        raise ValueError(f"y muss [N] sein und zu X passen. X:{X.shape} y:{y.shape}")
    return X, y


def try_load_classnames(meta_path):
    if not meta_path or not os.path.isfile(meta_path):
        return None
    try:
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)
        classes = meta.get("classes", None)
        if isinstance(classes, list) and len(classes) > 0:
            return [str(x) for x in classes]
    except Exception:
        return None
    return None


def write_text(path, text):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(text)


def write_csv_matrix(path, cm, class_names_used):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        w.writerow([""] + list(class_names_used))
        for i, row in enumerate(cm):
            w.writerow([class_names_used[i]] + [int(x) for x in row])


def write_rows_csv(path, rows, fieldnames):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames)
        w.writeheader()
        for r in rows:
            w.writerow(r)


# =========================
# FEATURE ENGINEERING
# =========================
def add_magnitude_features(X):
    # X: [N,T,6] -> [N,T,8]
    if X.shape[2] != 6:
        raise ValueError("Magnitude-Features erwarten 6 Kanäle.")
    ax, ay, az = X[..., 0], X[..., 1], X[..., 2]
    gx, gy, gz = X[..., 3], X[..., 4], X[..., 5]

    acc_mag = np.sqrt(np.maximum(ax * ax + ay * ay + az * az, 0.0)).astype(np.float32)[..., None]
    gyr_mag = np.sqrt(np.maximum(gx * gx + gy * gy + gz * gz, 0.0)).astype(np.float32)[..., None]

    return np.concatenate([X, acc_mag, gyr_mag], axis=2).astype(np.float32, copy=False)


def add_jerk_magnitude_features(X):
    # X: [N,T,6] -> [N,T,10]
    X8 = add_magnitude_features(X)  # [N,T,8]
    d = np.diff(X[:, :, :6], axis=1, prepend=X[:, 0:1, :6]).astype(np.float32)

    dax, day, daz = d[..., 0], d[..., 1], d[..., 2]
    dgx, dgy, dgz = d[..., 3], d[..., 4], d[..., 5]

    acc_jerk = np.sqrt(np.maximum(dax * dax + day * day + daz * daz, 0.0)).astype(np.float32)[..., None]
    gyr_jerk = np.sqrt(np.maximum(dgx * dgx + dgy * dgy + dgz * dgz, 0.0)).astype(np.float32)[..., None]

    return np.concatenate([X8, acc_jerk, gyr_jerk], axis=2).astype(np.float32, copy=False)


def build_features(X, mode):
    if mode == "basic":
        return X.astype(np.float32, copy=False)
    if mode == "mag":
        return add_magnitude_features(X)
    if mode == "mag_jerk":
        return add_jerk_magnitude_features(X)
    raise ValueError("FEATURE_MODE muss 'basic', 'mag' oder 'mag_jerk' sein.")


# =========================
# CLASS WEIGHTS + FOCAL LOSS
# =========================
def compute_class_weights(y, num_classes):
    counts = np.bincount(y, minlength=num_classes).astype(np.float32)
    counts = np.maximum(counts, 1.0)
    inv = 1.0 / counts
    weights = inv * (counts.sum() / num_classes)
    return {i: float(weights[i]) for i in range(num_classes)}


@tf.keras.utils.register_keras_serializable()
class SparseFocalLoss(tf.keras.losses.Loss):
    def __init__(self, gamma=2.0, alpha=0.25, name="sparse_focal_loss"):
        super().__init__(name=name)
        self.gamma = float(gamma)
        self.alpha = float(alpha)

    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.int32)
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)

        idx = tf.stack([tf.range(tf.shape(y_true)[0]), y_true], axis=1)
        p_t = tf.gather_nd(y_pred, idx)

        focal = tf.pow(1.0 - p_t, self.gamma)
        loss = -self.alpha * focal * tf.math.log(p_t)
        return tf.reduce_mean(loss)

    def get_config(self):
        return {"gamma": self.gamma, "alpha": self.alpha, "name": self.name}


# =========================
# MODELL
# =========================
def build_tiny_sepconv(T, C, num_classes):
    inp = tf.keras.Input(shape=(T, C), name="imu_window")

    x = tf.keras.layers.SeparableConv1D(24, 7, padding="same", use_bias=False)(inp)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)
    x = tf.keras.layers.MaxPool1D(2)(x)

    x = tf.keras.layers.SeparableConv1D(32, 5, padding="same", use_bias=False)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)
    x = tf.keras.layers.MaxPool1D(2)(x)

    x = tf.keras.layers.SeparableConv1D(40, 3, padding="same", use_bias=False)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)

    x = tf.keras.layers.GlobalAveragePooling1D()(x)
    x = tf.keras.layers.Dense(40, activation="relu")(x)
    x = tf.keras.layers.Dropout(0.25)(x)

    out = tf.keras.layers.Dense(num_classes, activation="softmax", name="class_prob")(x)
    return tf.keras.Model(inp, out, name="tiny_sepconv_1d")


def export_float_tflite(model, out_path):
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    tflite_model = converter.convert()
    with open(out_path, "wb") as f:
        f.write(tflite_model)


# =========================
# EVALUATION (ROBUST GEGEN "13 vs 3" KLASSEN)
# =========================
def predict_classes(model, X, batch_size=256):
    probs = model.predict(X, batch_size=batch_size, verbose=0)
    return np.argmax(probs, axis=1).astype(np.int64)


def compute_metrics_robust(y_true, y_pred, class_names_all):
    labels_used = np.unique(np.concatenate([np.unique(y_true), np.unique(y_pred)])).astype(int)

    class_names_used = []
    for lab in labels_used:
        if lab < len(class_names_all):
            class_names_used.append(class_names_all[lab])
        else:
            class_names_used.append(f"Klasse_{lab}")

    acc = float(accuracy_score(y_true, y_pred))
    f1m = float(f1_score(y_true, y_pred, labels=labels_used, average="macro", zero_division=0))

    pr, rc, f1, supp = precision_recall_fscore_support(
        y_true, y_pred, labels=labels_used, zero_division=0
    )

    per_class = []
    for i, lab in enumerate(labels_used):
        per_class.append({
            "class_id": int(lab),
            "class_name": class_names_used[i],
            "precision": float(pr[i]),
            "recall": float(rc[i]),
            "f1": float(f1[i]),
            "support": int(supp[i]),
        })

    cm = confusion_matrix(y_true, y_pred, labels=labels_used).astype(int)

    return {
        "accuracy": acc,
        "f1_macro": f1m,
        "labels_used": labels_used.tolist(),
        "class_names_used": class_names_used,
        "per_class": per_class,
        "confusion_matrix": cm,
    }


# =========================
# HARD TESTS
# =========================
def hardtest_apply(X, rng, mode, strength):
    X2 = X.copy()
    N, T, C = X2.shape

    if mode == "jitter":
        sigma = float(strength)
        X2 += rng.normal(0.0, sigma, size=X2.shape).astype(np.float32)

    elif mode == "scaling":
        scale_sigma = float(strength)
        gains = (1.0 + rng.normal(0.0, scale_sigma, size=(1, 1, C))).astype(np.float32)
        X2 *= gains

    elif mode == "timeshift":
        max_shift = max(1, int(round(float(strength) * T)))
        shift = int(rng.integers(-max_shift, max_shift + 1))
        X2 = np.roll(X2, shift=shift, axis=1).astype(np.float32)

    elif mode == "dropout_block":
        block = max(1, min(int(round(float(strength) * T)), T - 1))
        start = int(rng.integers(0, T - block))
        X2[:, start:start + block, :] = 0.0

    else:
        raise ValueError(f"Unbekannter Hardtest-Mode: {mode}")

    return X2.astype(np.float32, copy=False)


def run_hardtests(model, X_test, y_test, seed, batch_size):
    rng = np.random.default_rng(seed)
    tests = [
        ("baseline", None, 0.0),
        ("jitter_low", "jitter", 0.03),
        ("jitter_med", "jitter", 0.06),
        ("scaling_low", "scaling", 0.05),
        ("scaling_med", "scaling", 0.10),
        ("timeshift_5pct", "timeshift", 0.05),
        ("timeshift_10pct", "timeshift", 0.10),
        ("dropout_block_5pct", "dropout_block", 0.05),
        ("dropout_block_10pct", "dropout_block", 0.10),
    ]

    rows = []
    for name, mode, strength in tests:
        X_use = X_test if mode is None else hardtest_apply(X_test, rng, mode, strength)
        y_pred = predict_classes(model, X_use, batch_size=batch_size)

        rows.append({
            "test": name,
            "mode": mode if mode else "none",
            "strength": float(strength),
            "accuracy": float(accuracy_score(y_test, y_pred)),
            "f1_macro": float(f1_score(y_test, y_pred, average="macro")),
        })

    return rows


# =========================
# MAIN
# =========================
def set_seed(seed):
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)


def main():
    print("Starte Step 6: Optimieren & Hard Test")
    print("IN_DIR :", os.path.abspath(IN_DIR))
    print("OUT_DIR:", os.path.abspath(OUT_DIR))
    print("FEATURE_MODE:", FEATURE_MODE)
    print("Training: epochs =", EPOCHS, "| batch =", BATCH_SIZE, "| lr =", LR)
    print("USE_FOCAL:", USE_FOCAL)

    os.makedirs(OUT_DIR, exist_ok=True)
    set_seed(SEED)

    # Daten laden
    X_train, y_train = load_npz(os.path.join(IN_DIR, "train_aug_x3.npz"))
    X_val, y_val = load_npz(os.path.join(IN_DIR, "val_norm.npz"))
    X_test, y_test = load_npz(os.path.join(IN_DIR, "test_norm.npz"))

    print("\nDaten geladen:")
    print("  Train:", X_train.shape, y_train.shape)
    print("  Val  :", X_val.shape, y_val.shape)
    print("  Test :", X_test.shape, y_test.shape)

    # Features
    X_train_f = build_features(X_train, FEATURE_MODE)
    X_val_f = build_features(X_val, FEATURE_MODE)
    X_test_f = build_features(X_test, FEATURE_MODE)

    print("\nNach Feature Engineering:")
    print("  Train features:", X_train_f.shape)
    print("  Val features  :", X_val_f.shape)
    print("  Test features :", X_test_f.shape)

    # Klassen
    num_classes = int(max(y_train.max(), y_val.max(), y_test.max()) + 1)
    class_names_all = try_load_classnames(STEP1_META)
    if class_names_all is None:
        class_names_all = [f"Klasse_{i}" for i in range(num_classes)]

    print("\nKlassen:")
    print("  num_classes (aus Labels):", num_classes)
    print("  metadata classes length :", len(class_names_all))
    print("  y_train unique:", np.unique(y_train))
    print("  y_val unique  :", np.unique(y_val))
    print("  y_test unique :", np.unique(y_test))

    # Class weights
    class_weights = compute_class_weights(y_train, num_classes)
    print("\nClass weights (train):")
    for k in sorted(class_weights.keys()):
        print(f"  {k}: {class_weights[k]:.3f}")

    # Modell
    T, C = X_train_f.shape[1], X_train_f.shape[2]
    model = build_tiny_sepconv(T, C, num_classes)

    loss = SparseFocalLoss(gamma=FOCAL_GAMMA, alpha=FOCAL_ALPHA) if USE_FOCAL else "sparse_categorical_crossentropy"

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LR),
        loss=loss,
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name="acc")]
    )

    model.summary()

    # Callbacks
    best_path = os.path.join(OUT_DIR, "best_model.keras")
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(best_path, monitor="val_acc", mode="max", save_best_only=True, verbose=1),
        tf.keras.callbacks.EarlyStopping(monitor="val_acc", mode="max", patience=15, restore_best_weights=True, verbose=1),
        tf.keras.callbacks.ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=6, min_lr=1e-5, verbose=1),
    ]

    # Train
    history = model.fit(
        X_train_f, y_train,
        validation_data=(X_val_f, y_val),
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        shuffle=True,
        class_weight=class_weights,
        callbacks=callbacks,
        verbose=2
    )

    # Eval
    y_val_pred = predict_classes(model, X_val_f, batch_size=PRED_BATCH)
    y_test_pred = predict_classes(model, X_test_f, batch_size=PRED_BATCH)

    val_metrics = compute_metrics_robust(y_val, y_val_pred, class_names_all)
    test_metrics = compute_metrics_robust(y_test, y_test_pred, class_names_all)

    print("\nErgebnis:")
    print(f"  Val  Accuracy={val_metrics['accuracy']:.4f} | F1_macro={val_metrics['f1_macro']:.4f}")
    print(f"  Test Accuracy={test_metrics['accuracy']:.4f} | F1_macro={test_metrics['f1_macro']:.4f}")

    # Confusion matrix CSV (nur labels_used)
    write_csv_matrix(
        os.path.join(OUT_DIR, "confusion_matrix.csv"),
        test_metrics["confusion_matrix"],
        test_metrics["class_names_used"]
    )

    # Hardtests
    hardtests = run_hardtests(model, X_test_f, y_test, seed=HARDTEST_SEED, batch_size=PRED_BATCH)
    write_rows_csv(
        os.path.join(OUT_DIR, "hardtests.csv"),
        hardtests,
        fieldnames=["test", "mode", "strength", "accuracy", "f1_macro"]
    )

    # Export TFLite
    export_float_tflite(model, os.path.join(OUT_DIR, "model_float.tflite"))

    # Save metrics.json
    metrics = {
        "feature_mode": FEATURE_MODE,
        "num_classes": int(num_classes),
        "train": {
            "seed": int(SEED),
            "epochs": int(EPOCHS),
            "batch_size": int(BATCH_SIZE),
            "lr": float(LR),
            "use_focal": bool(USE_FOCAL),
            "focal_gamma": float(FOCAL_GAMMA),
            "focal_alpha": float(FOCAL_ALPHA),
            "class_weights": class_weights,
        },
        "val": {
            "accuracy": val_metrics["accuracy"],
            "f1_macro": val_metrics["f1_macro"],
            "labels_used": val_metrics["labels_used"],
            "class_names_used": val_metrics["class_names_used"],
            "per_class": val_metrics["per_class"],
        },
        "test": {
            "accuracy": test_metrics["accuracy"],
            "f1_macro": test_metrics["f1_macro"],
            "labels_used": test_metrics["labels_used"],
            "class_names_used": test_metrics["class_names_used"],
            "per_class": test_metrics["per_class"],
        },
        "hardtests": hardtests,
        "history": {k: [float(x) for x in v] for k, v in history.history.items()},
    }

    with open(os.path.join(OUT_DIR, "metrics.json"), "w", encoding="utf-8") as f:
        json.dump(metrics, f, ensure_ascii=False, indent=2)

    # Deutscher Report
    val_acc = history.history.get("val_acc", [])
    tr_acc = history.history.get("acc", [])
    best_epoch = int(np.argmax(val_acc)) if len(val_acc) else -1
    best_val = float(val_acc[best_epoch]) if best_epoch >= 0 else float("nan")
    tr_at_best = float(tr_acc[best_epoch]) if (best_epoch >= 0 and best_epoch < len(tr_acc)) else float("nan")
    gap = tr_at_best - best_val if np.isfinite(best_val) and np.isfinite(tr_at_best) else float("nan")

    lines = []
    lines.append("Step 6 Report: Optimierung & Hard Test")
    lines.append("")
    lines.append(f"Feature Mode: {FEATURE_MODE}")
    lines.append(f"Train: epochs={EPOCHS}, batch={BATCH_SIZE}, lr={LR}")
    lines.append(f"Loss: {'Focal' if USE_FOCAL else 'CrossEntropy'}")
    lines.append("")
    if best_epoch >= 0:
        lines.append("Overfitting (Daumenwerte):")
        lines.append(f"- Beste Epoche: {best_epoch + 1}")
        lines.append(f"- Val-Acc best: {best_val:.4f}")
        lines.append(f"- Train-Acc an best: {tr_at_best:.4f}")
        lines.append(f"- Gap (Train-Val): {gap:.4f}")
        lines.append("")
    lines.append("Val/Test:")
    lines.append(f"- Val  Accuracy={val_metrics['accuracy']:.4f}, F1_macro={val_metrics['f1_macro']:.4f}")
    lines.append(f"- Test Accuracy={test_metrics['accuracy']:.4f}, F1_macro={test_metrics['f1_macro']:.4f}")
    lines.append("")
    lines.append("Hard Tests (niedrigste Accuracy zuerst):")
    for r in sorted(hardtests, key=lambda x: x["accuracy"]):
        lines.append(f"- {r['test']}: Accuracy={r['accuracy']:.4f}, F1_macro={r['f1_macro']:.4f}")

    lines.append("")
    lines.append("Hinweis:")
    lines.append("Wenn bestimmte Klassen (z.B. mobalat vs trabi) verwechselt werden,")
    lines.append("lohnt sich meistens: mehr Daten, sauberere Labels, und leicht andere Fensterlängen.")

    write_text(os.path.join(OUT_DIR, "report_de.txt"), "\n".join(lines))

    print("\nFERTIG ")
    print("Outputs:", os.path.abspath(OUT_DIR))
    print("  best_model.keras")
    print("  model_float.tflite")
    print("  report_de.txt")
    print("  confusion_matrix.csv")
    print("  hardtests.csv")
    print("  metrics.json")


if __name__ == "__main__":
    main()
