import os
import json
import numpy as np
import pandas as pd
import joblib

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ============================================================
# STEP 6: "Quantisierung" für Decision Tree (scikit-learn)
# - FLOAT Eval (normale Features)
# - INT8 Feature Quantisierung (per Feature scale) + Eval
# - leakage-free split by base_id
# ============================================================

# =============================
# PATHS (KORREKT)
# =============================
STEP3_DIR = r"C:\Users\Oulad\Downloads\Devi\Devi_step3"
STEP4_DIR = r"C:\Users\Oulad\Downloads\Devi\Devi_step4_dt_tuned"

MODEL_PATH = os.path.join(STEP4_DIR, "decision_tree_tuned.joblib")

OUT_DIR = r"C:\Users\Oulad\Downloads\Devi\Devi_step6_dt_quant"
if not os.path.exists(OUT_DIR):
    os.makedirs(OUT_DIR)

QPARAMS_PATH = os.path.join(OUT_DIR, "quant_params.json")

# =============================
# SETTINGS (muss gleich wie Training sein)
# =============================
BASE_COLS = ["AX(g)", "AY(g)", "AZ(g)", "GX(deg/s)", "GY(deg/s)", "GZ(deg/s)"]
WINDOW_SIZE = 256
STRIDE = 128

# =============================
# FEATURES (Magnitude + Jerk) - exakt wie Training
# =============================
def get_base_id(filename):
    return filename.replace(".csv", "").split("__")[0]

def features_1d(x):
    x = np.asarray(x, dtype=float)
    mean = np.mean(x)
    std = np.std(x)  # ddof=0
    mn = np.min(x)
    mx = np.max(x)
    rms = np.sqrt(np.mean(x * x))
    med = np.median(x)
    q25 = np.percentile(x, 25)
    q75 = np.percentile(x, 75)
    iqr = q75 - q25
    energy = np.mean(x * x)
    return [mean, std, mn, mx, rms, med, iqr, energy]

feat_names_1d = ["mean", "std", "min", "max", "rms", "median", "iqr", "energy"]

def build_channels(df):
    AX = df["AX(g)"].astype(float).to_numpy()
    AY = df["AY(g)"].astype(float).to_numpy()
    AZ = df["AZ(g)"].astype(float).to_numpy()
    GX = df["GX(deg/s)"].astype(float).to_numpy()
    GY = df["GY(deg/s)"].astype(float).to_numpy()
    GZ = df["GZ(deg/s)"].astype(float).to_numpy()

    acc_mag = np.sqrt(AX*AX + AY*AY + AZ*AZ)
    gyro_mag = np.sqrt(GX*GX + GY*GY + GZ*GZ)

    chans = {
        "AX": AX, "AY": AY, "AZ": AZ,
        "GX": GX, "GY": GY, "GZ": GZ,
        "acc_mag": acc_mag,
        "gyro_mag": gyro_mag
    }

    # jerk
    for k in list(chans.keys()):
        x = chans[k]
        chans[k + "_jerk"] = np.diff(x, prepend=x[0])

    return chans

def make_feature_order():
    # muss gleiche Reihenfolge sein wie im Training-Code
    base_names = ["AX", "AY", "AZ", "GX", "GY", "GZ", "acc_mag", "gyro_mag"]
    jerk_names = [n + "_jerk" for n in base_names]
    ch_names = base_names + jerk_names

    feature_names = []
    for cn in ch_names:
        for fn in feat_names_1d:
            feature_names.append(cn + "_" + fn)

    return ch_names, feature_names

CHANNEL_NAMES, FEATURE_NAMES = make_feature_order()

def load_windows_df(step3_dir):
    rows = []
    for label in os.listdir(step3_dir):
        label_path = os.path.join(step3_dir, label)
        if not os.path.isdir(label_path):
            continue

        for file in os.listdir(label_path):
            if not file.endswith(".csv"):
                continue

            fp = os.path.join(label_path, file)
            try:
                df = pd.read_csv(fp)
            except:
                continue

            ok = True
            for c in BASE_COLS:
                if c not in df.columns:
                    ok = False
            if not ok:
                continue

            base_id = get_base_id(file)
            chans = build_channels(df)

            n = len(df)
            if n < WINDOW_SIZE:
                continue

            start = 0
            while start + WINDOW_SIZE <= n:
                feats = []
                for cn in CHANNEL_NAMES:
                    x = chans[cn][start:start + WINDOW_SIZE]
                    feats += features_1d(x)

                rows.append([label, base_id] + feats)
                start += STRIDE

    if len(rows) == 0:
        return None

    cols = ["label", "base_id"] + FEATURE_NAMES
    return pd.DataFrame(rows, columns=cols)

# =============================
# INT8 Quantization (per Feature)
# =============================
def compute_int8_params(X_train):
    mins = X_train.min(axis=0)
    maxs = X_train.max(axis=0)

    max_abs = np.maximum(np.abs(mins), np.abs(maxs))
    scale = max_abs / 127.0
    scale = np.maximum(scale, 1e-8).astype(np.float32)

    zero_point = np.zeros_like(scale, dtype=np.int32)  # symmetric
    return scale, zero_point

def quantize_int8(X, scale, zero_point):
    x_q = np.round(X / scale + zero_point).astype(np.int32)
    x_q = np.clip(x_q, -128, 127).astype(np.int8)
    return x_q

def dequantize_int8(Xq, scale, zero_point):
    return (Xq.astype(np.float32) - zero_point) * scale

# =============================
# MAIN
# =============================
print("Loading windows from Step3...")
df_all = load_windows_df(STEP3_DIR)
if df_all is None or len(df_all) == 0:
    raise SystemExit("❌ No samples found. Check STEP3_DIR/WINDOW_SIZE.")

print("Total windows:", len(df_all))

# leakage-free split by base_id
unique_bases = df_all[["base_id", "label"]].drop_duplicates()
train_bases, test_bases = train_test_split(
    unique_bases, test_size=0.2, random_state=42, stratify=unique_bases["label"]
)

train_ids = set(train_bases["base_id"].tolist())
test_ids = set(test_bases["base_id"].tolist())
print("Leakage check (muss 0 sein):", len(train_ids.intersection(test_ids)))

train_df = df_all[df_all["base_id"].isin(train_ids)]
test_df = df_all[df_all["base_id"].isin(test_ids)]

X_train = train_df[FEATURE_NAMES].to_numpy(dtype=np.float32)
y_train = train_df["label"].to_numpy()

X_test = test_df[FEATURE_NAMES].to_numpy(dtype=np.float32)
y_test = test_df["label"].to_numpy()

print("Train windows:", len(X_train), "Test windows:", len(X_test))

print("\nModel path:", MODEL_PATH)
print("MODEL exists:", os.path.exists(MODEL_PATH))
if not os.path.exists(MODEL_PATH):
    raise SystemExit("❌ Model file not found. Check STEP4_DIR / MODEL_PATH.")

print("\nLoading Decision Tree model...")
model = joblib.load(MODEL_PATH)

# =============================
# FLOAT EVAL
# =============================
print("\n===== FLOAT EVAL =====")
pred_float = model.predict(X_test)
acc_float = accuracy_score(y_test, pred_float)
print("✅ Float Window-Accuracy:", acc_float)
print("\nClassification Report (float):")
print(classification_report(y_test, pred_float))
print("Confusion Matrix (float):")
print(confusion_matrix(y_test, pred_float))

# =============================
# INT8 FEATURE EVAL
# =============================
scale, zero_point = compute_int8_params(X_train)

params = {
    "feature_names": FEATURE_NAMES,
    "scale": scale.tolist(),
    "zero_point": zero_point.tolist(),
    "note": "symmetric int8 per feature: x_q=round(x/scale), clamp [-128,127], x=(x_q-zp)*scale"
}
with open(QPARAMS_PATH, "w", encoding="utf-8") as f:
    json.dump(params, f, indent=2)

print("\nSaved quant params:", QPARAMS_PATH)

Xq_test = quantize_int8(X_test, scale, zero_point)
Xdq_test = dequantize_int8(Xq_test, scale, zero_point)

print("\n===== INT8 FEATURE EVAL =====")
pred_int8 = model.predict(Xdq_test)
acc_int8 = accuracy_score(y_test, pred_int8)
print("✅ Int8-feature Window-Accuracy:", acc_int8)
print("\nClassification Report (int8 features):")
print(classification_report(y_test, pred_int8))
print("Confusion Matrix (int8 features):")
print(confusion_matrix(y_test, pred_int8))

print("\n✅ Step 6 fertig (Decision Tree Quantisierung + Evaluation)")

